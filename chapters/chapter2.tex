
\chapter{Related Literature and Studies}
\begin{refsection}

The process of data collection began with analysis of the physical principles underlying optical light emission. For illustration purposes, see \ref{fig:secondFig}.

\section{Review of Related Literature and Studies}


\incite{Ogundokun2022} used a dimensionality reduction approach to decrease the dimensionality of the features that were not specified in the study, using a Genetic Algorithm (GA) for feature selection and machine learning classifiers for classification. The GA was applied to the dataset, and the resulting subset was passed to two classifiers: K-Nearest Neighbors (KNN) and an Ensemble classifier. The GA + KNN combination outperformed the GA + Ensemble in terms of accuracy, sensitivity, and F-measure, achieving a maximum accuracy of 99.28\%. However, the Ensemble method scored 100\% in precision and specificity. The study suggests that more dimensionality reduction techniques and additional machine learning classifiers could further improve fake news classification.

\incite{senthilkumar_comparative_2023}presents a model for detecting fake news using machine learning techniques and natural language processing. The model, created in Python, uses a deep diffusion network to learn representations of news articles, creators, and subjects. The Passive Aggressive Classifier, a machine learning algorithm, is proposed for fake news detection and outperforms other methods in terms of accuracy. The study contributes to the ongoing “Fake News Challenge,” a Kaggle competition aimed at using AI to filter fake news. The proposed system demonstrated improved accuracy and comparability, and experimental studies showed it significantly outperforms the state-of-the-art in both late and early detection settings.

\incite{maheswari_efficient_2024}The study uses a three-step method involving pre-processing, feature extraction using the Lexicon Model, and feature selection using optimization algorithms. It proposes a Hybrid Classification model, combining a Convolutional Neural Network and a support vector machine, and an improved model using an ensemble of Recurrent Neural Networks (RNN), VGG-16, and ResNet50. The proposed model achieved an accuracy of 96.23\%, outperforming other models. However, due to the high computational complexity of deep learning, there’s a need for other models in the future.

\incite{pande_fake_2022}proposed automated detection processes using machine learning techniques, including logistic regression, word embedding, and Long Short-Term Memory (LSTM). The methods involve data cleaning, categorization, and classification. The model was capable of classifying news as fake or not. The throughput of the model is further evaluated based on the confusion matrix, with a total of 8980 samples used in which the model has given correct output of 8719 samples and 261 were incorrect. Making the model 97.09\% accurate. Where 120 instances are true negatives, 4574 are true positives, 141 are false positives and 4145 are false negatives.

\incite{CHAUHAN2021100051}used a deep learning-based model with LSTM architecture and GloVe for word embeddings. The study aimed to benefit society and the government by reducing the spread of fake news. The proposed model, which used data pre-processing and tokenization for feature extraction, achieved an accuracy of 99.88\%.
The study suggested that the model can be improved and expanded for use in e-commerce websites where fake news detection is also crucial.


\incite{balpande_fake_2021}used two classification model which is Naïve Bayes and TF - IDF VEctorizer. The aim of this paper is to try to tackle the growing problems with pretend news, which has been continuously been a retardant by the widespread use of social media. However, a huge quantity of knowledge is generated in social media and pretend news news comes back from the information, misunderstanding or unreliable contents with the creditability supply. This makes the news hard to believe.



%=======================================================%
%%%%% Do not delete this part %%%%%%
\clearpage

\printbibliography[heading=subbibintoc, title={\centering Notes}]
\end{refsection}